---
sidebar: sidebar 
permalink: technical-reports/bare-metal/prerequisites-install-storagegrid.html 
keywords: prerequisites, install storagegrid 
summary: StorageGRIDを導入するためのコンピューティング、ストレージ、ネットワーク、Docker、ノードの要件について説明します。 
---
= StorageGRIDをインストールするための前提条件
:hardbreaks:
:allow-uri-read: 
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ../../media/


[role="lead"]
StorageGRIDを導入するためのコンピューティング、ストレージ、ネットワーク、Docker、ノードの要件について説明します。



== コンピューティング要件

次の表に、StorageGRIDノードのタイプごとにサポートされる最小リソース要件を示します。これらは、StorageGRIDノードに必要な最小限のリソースです。

[cols="30,35,35"]
|===
| ノードのタイプ | CPUコア数 | RAM 


| 管理者 | 8 | 24 GB 


| ストレージ | 8 | 24 GB 


| ゲートウェイ | 8 | 24 GB 
|===
また、適切に動作するためには、各物理Dockerホストに16GB以上のRAMを割り当てる必要があります。たとえば、表に記載されている2つのサービスを1つの物理Dockerホストで一緒にホストするには、次の計算を行います。

24 + 24 + 16 = 64GB RAM、8 + 8 = 16コア

最新のサーバの多くはこれらの要件を超えているため、6つのサービス（StorageGRIDコンテナ）を3台の物理サーバに統合しました。



== ネットワーク要件

StorageGRIDトラフィックには、次の3種類があります。

* *グリッドトラフィック（必須）。*グリッド内のすべてのノードの間で伝送される、内部 StorageGRID トラフィック。
* *管理トラフィック（オプション）。*システムの管理とメンテナンスに使用されるトラフィック。
* *クライアントトラフィック（オプション）。*S3 および Swift クライアントからのオブジェクトストレージ要求をすべて含む、外部のクライアントアプリケーションとグリッドの間で伝送されるトラフィック。


StorageGRIDシステムで使用するネットワークを3つまで設定できます。各ネットワークタイプは、重複のない別 々 のサブネット上に存在する必要があります。すべてのノードが同じサブネット上にある場合、ゲートウェイアドレスは必要ありません。

この評価では、グリッドトラフィックとクライアントトラフィックを含む2つのネットワークにを導入します。あとで管理ネットワークを追加して、その機能を利用することもできます。

すべてのホストのインターフェイスにネットワークを一貫してマッピングすることが非常に重要です。たとえば、各ノードにens192とens224の2つのインターフェイスがある場合は、すべてのホストで同じネットワークまたはVLANにマッピングする必要があります。このインストールでは、インストーラはこれらをeth0@if2およびeth2@if3としてDockerコンテナにマッピングします（ループバックはコンテナ内のIf1であるため）。したがって、一貫したモデルが非常に重要です。



=== Dockerネットワークに関する注意事項

StorageGRIDでは、一部のDockerコンテナ実装とは異なるネットワークを使用します。Docker（Kubernetes、Swarm）が提供するネットワークは使用しません。代わりに、StorageGRIDは実際には-- net=noneとしてコンテナを生成し、Dockerはコンテナのネットワーク化に何もしないようにします。StorageGRIDサービスによってコンテナが生成されると、ノード構成ファイルに定義されているインターフェイスから新しいmacvlanデバイスが作成されます。このデバイスは新しいMACアドレスを持ち、物理インターフェイスからパケットを受信できる別個のネットワークデバイスとして機能します。macvlanデバイスはコンテナネームスペースに移動され、コンテナ内のeth0、eth1、またはeth2のいずれかに名前が変更されます。この時点で、ネットワークデバイスはホストOSに表示されなくなります。この例では、Dockerコンテナ内のグリッドネットワークデバイスはeth0で、クライアントネットワークはeth2です。管理ネットワークがある場合、デバイスはコンテナ内のeth1になります。


NOTE: コンテナネットワークデバイスの新しいMACアドレスでは、一部のネットワーク環境および仮想環境で無差別モードを有効にする必要がある場合があります。このモードでは、物理デバイスは既知の物理MACアドレスとは異なるMACアドレスのパケットを送受信できます。++ VMware vSphereで実行している場合は、RHELの実行時にStorageGRIDトラフィックを処理するポートグループで、プロミスキャスモード、MACアドレスの変更、および偽装送信を受け入れる必要があります。UbuntuまたはDebianはほとんどの状況でこれらの変更なしに動作します。+++++



== ストレージ要件

各ノードには、次の表に示すサイズのSANベースまたはローカルディスクデバイスが必要です。


NOTE: 表内の数値はStorageGRIDサービスタイプごとのものであり、グリッド全体や物理ホストごとの数値ではありません。導入の選択肢に基づいて、このドキュメントで後述するでの各物理ホストの数を計算します link:prerequisites-install-storagegrid.html#physical-host-layout-and-requirements["物理ホストのレイアウトと要件"]。++アスタリスクが付いているパスまたはファイルシステムは、インストーラによってStorageGRIDコンテナ自体に作成されます。管理者による手動での設定やファイルシステムの作成は必要ありませんが、これらの要件を満たすためにはホストにブロックデバイスが必要です。つまり、ブロックデバイスはコマンドを使用して表示され `lsblk` ますが、ホストOS内でフォーマットまたはマウントされていません。+++++

[cols="15,20,15,15,15,20"]
|===
| ノードタイプ | LUNの用途 | LUN数 | LUNの最小サイズ | 手動ファイルシステムが必要 | 推奨されるノード設定エントリ 


| すべて | 管理ノードのシステムスペース
`/var/local` （SSDが有用） | 管理ノードごとに1つ | 90GB | いいえ | `BLOCK_DEVICE_VAR_LOCAL = /dev/mapper/ADM-VAR-LOCAL` 


| すべてのノード | Dockerストレージプール：
`/var/lib/docker for container pool` | ホスト（物理またはVM）ごとに1つ | コンテナあたり100GB | ○–etx4 | NA–フォーマットしてホストファイルシステムとしてマウント（コンテナにマッピングされていない） 


| 管理者 | 管理ノードの監査ログ（管理コンテナ内のシステムデータ）
`/var/local/audit/export` | 管理ノードごとに1つ | 200GB | いいえ | `BLOCK_DEVICE_AUDIT_LOGS =/dev/mapper/ADM-OS` 


| 管理者 | 管理ノードのテーブル（管理コンテナ内のシステムデータ）
`/var/local/mysql_ibdata` | 管理ノードごとに1つ | 200GB | いいえ | `BLOCK_DEVICE_TABLES = /dev/mapper/ADM-MySQL` 


| ストレージノード | オブジェクトストレージ（ブロックデバイス）  `/var/local/rangedb0` （SSDが役立つ）  `/var/local/rangedb1`  `/var/local/rangedb2` | ストレージコンテナごとに3つ | 4000GB | いいえ | `BLOCK_DEVICE_RANGEDB_000 = /dev/mapper/SN-Db00
BLOCK_DEVICE_RANGEDB_001 = /dev/mapper/SN-Db01
BLOCK_DEVICE_RANGEDB_002 = /dev/mapper/SN-Db02` 
|===
この例では、コンテナタイプごとに必要なディスクサイズを次の表に示します。物理ホストごとの要件については、このドキュメントの後半で説明し link:prerequisites-install-storagegrid.html#physical-host-layout-and-requirements["物理ホストのレイアウトと要件"]ます。



== コンテナタイプ別のディスクサイズ



=== Adminコンテナ

[cols="50,50"]
|===
| 名前 | サイズ（GiB） 


| Dockerストア | 100（コンテナあたり） 


| ADM-OS | 90 


| ADM -監査 | 200です 


| ADM - MySQL | 200です 
|===


=== ストレージコンテナ

[cols="50,50"]
|===
| 名前 | サイズ（GiB） 


| Dockerストア | 100（コンテナあたり） 


| SN-OS | 90 


| Rangedb-0 | 4096 


| Rangedb-1 | 4096 


| Rangedb-2 | 4096 
|===


=== ゲートウェイコンテナ

[cols="50,50"]
|===
| 名前 | サイズ（GiB） 


| Dockerストア | 100（コンテナあたり） 


| /var/local | 90 
|===


== 物理ホストのレイアウトと要件

上記の表に示すコンピューティング要件とネットワーク要件を組み合わせることで、16コア、64GBのRAM、2つのネットワークインターフェイスを備えた3台の物理（または仮想）サーバに必要な基本的なハードウェアセットを入手できます。より高いスループットが必要な場合は、グリッドネットワークまたはクライアントネットワーク上の複数のインターフェイスをボンディングし、ノード構成ファイルでbond0.520などのVLANタグ付きインターフェイスを使用できます。負荷の高いワークロードが必要な場合は、ホストとコンテナの両方のメモリを増やす方が効果的です。

次の図に示すように、これらのサーバは6つのDockerコンテナ（ホストごとに2つ）をホストします。RAMはコンテナあたり24GB、ホストOS自体に16GBを提供することで計算されます。

image:bare-metal-layout-for-three-hosts.png["3台のホストのレイアウト例"]

物理ホスト（VM）あたりに必要な合計RAMは、24 x 2 + 16 = 64GBです。次の表に、ホスト1、2、3に必要なディスクストレージを示します。

[cols="50,50"]
|===
| ホスト1 | サイズ（GiB） 


 a| 
* Dockerストア*



| `/var/lib/docker` （ファイルシステム） | 200（100 x 2） 


 a| 
*管理コンテナ*



| `BLOCK_DEVICE_VAR_LOCAL` | 90 


| `BLOCK_DEVICE_AUDIT_LOGS` | 200です 


| `BLOCK_DEVICE_TABLES` | 200です 


 a| 
*ストレージコンテナ*



| SN-OS
`/var/local` （デバイス） | 90 


| Rangedb-0（デバイス） | 4096 


| Rangedb-1（デバイス） | 4096 


| Rangedb-2（デバイス） | 4096 
|===
[cols="50,50"]
|===
| ホスト2 | サイズ（GiB） 


 a| 
* Dockerストア*



| `/var/lib/docker` （共有） | 200（100 x 2） 


 a| 
*ゲートウェイコンテナ*



| GW-OS *`/var/local` | 100 


 a| 
*ストレージコンテナ*



| *`/var/local` | 100 


| Rangedb-0 | 4096 


| Rangedb-1 | 4096 


| Rangedb-2 | 4096 
|===
[cols="50,50"]
|===
| ホスト3 | サイズ（GiB） 


 a| 
* Dockerストア*



| `/var/lib/docker` （共有） | 200（100 x 2） 


 a| 
*ゲートウェイコンテナ*



| *`/var/local` | 100 


 a| 
*ストレージコンテナ*



| *`/var/local` | 100 


| Rangedb-0 | 4096 


| Rangedb-1 | 4096 


| Rangedb-2 | 4096 
|===
Docker Storeの計算では、/var/localあたり100GB（コンテナあたり）x 2つのコンテナが200GBであるとしました。



== ノードの準備

StorageGRIDの初期インストールの準備として、まずRHELバージョン9.2をインストールし、SSHを有効にします。ベストプラクティスに従って、ネットワークインターフェイス、ネットワークタイムプロトコル（NTP）、DNS、およびホスト名を設定します。グリッドネットワークでとクライアントネットワーク用に少なくとも1つのネットワークインターフェイスが有効になっている必要があります。VLANタグ付きインターフェイスを使用している場合は、次の例に従って設定します。それ以外の場合は、シンプルな標準ネットワークインターフェイス設定で十分です。

グリッドネットワークインターフェイスでVLANタグを使用する必要がある場合は、次の形式の2つのファイルが構成に含まれている必要があります `/etc/sysconfig/network-scripts/` 。

[listing]
----
# cat /etc/sysconfig/network-scripts/ifcfg-enp67s0
# This is the parent physical device
TYPE=Ethernet
BOOTPROTO=none
DEVICE=enp67s0
ONBOOT=yes
# cat /etc/sysconfig/network-scripts/ifcfg-enp67s0.520
# The actual device that will be used by the storage node file
DEVICE=enp67s0.520
BOOTPROTO=none
NAME=enp67s0.520
IPADDR=10.10.200.31
PREFIX=24
VLAN=yes
ONBOOT=yes
----
この例では、グリッドネットワークの物理ネットワークデバイスがenp67s0であると想定しています。また、bond0などの結合デバイスにすることもできます。ネットワークポートにデフォルトのVLANがない場合やデフォルトのVLANがグリッドネットワークに関連付けられていない場合は、ボンディングを使用するか標準のネットワークインターフェイスを使用する必要があります。StorageGRIDコンテナ自体はイーサネットフレームのタグを解除しないため、親OSで処理する必要があります。



== iSCSIを使用したストレージセットアップ（オプション）

iSCSIストレージを使用しない場合は、host1、host2、およびhost3に、要件を満たす十分なサイズのブロックデバイスが含まれていることを確認する必要があります。host1、host2、およびhost3のストレージ要件については、を参照してください link:prerequisites-install-storagegrid.html#disk-sizes-per-container-type["コンテナタイプ別のディスクサイズ"] 。

iSCSIを使用してストレージをセットアップするには、次の手順を実行します。

.手順
. NetApp EシリーズやNetApp ONTAP®データ管理ソフトウェアなどの外部iSCSIストレージを使用する場合は、次のパッケージをインストールします。
+
[listing]
----
sudo yum install iscsi-initiator-utils
sudo yum install device-mapper-multipath
----
. 各ホストでイニシエータIDを確認します。
+
[listing]
----
# cat /etc/iscsi/initiatorname.iscsi
InitiatorName=iqn.2006-04.com.example.node1
----
. 手順2のイニシエータ名を使用して、ストレージデバイス上のLUN（表に示されている数とサイズ）を各ストレージノードにマッピングし link:prerequisites-install-storagegrid.html#storage-requirements["ストレージ要件"] ます。
. で新しく作成したLUNを検出し `iscsiadm` 、ログインします。
+
[listing]
----
# iscsiadm -m discovery -t st -p target-ip-address
# iscsiadm -m node -T iqn.2006-04.com.example:3260 -l
Logging in to [iface: default, target: iqn.2006-04.com.example:3260, portal: 10.64.24.179,3260] (multiple)
Login to [iface: default, target: iqn.2006-04.com.example:3260, portal: 10.64.24.179,3260] successful.
----
+

NOTE: 詳細については、Red Hatカスタマーポータルのを参照してください https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/storage_administration_guide/osm-create-iscsi-initiator["iSCSIイニシエータの作成"^] 。

. マルチパスデバイスとそれに関連付けられたLUN WWIDを表示するには、次のコマンドを実行します。
+
[listing]
----
# multipath -ll
----
+
iSCSIをマルチパスデバイスで使用していない場合は、一意のパス名を使用してデバイスをマウントするだけで、デバイスの変更やリブートが同じように維持されます。

+
[listing]
----
/dev/disk/by-path/pci-0000:03:00.0-scsi-0:0:1:0
----
+

TIP: デバイス名を使用するだけで、 `/dev/sdx` デバイスが削除または追加された場合に問題が発生する可能性があります。++マルチパスデバイスを使用している場合は、次のようにエイリアスを使用するようにファイルを変更します `/etc/multipath.conf` 。+++++

+

NOTE: レイアウトによっては、これらのデバイスがすべてのノードに存在する場合とない場合があります。

+
[listing]
----
multipaths {
multipath {
wwid 36d039ea00005f06a000003c45fa8f3dc
alias Docker-Store
}
multipath {
wwid 36d039ea00006891b000004025fa8f597
alias Adm-Audit
}
multipath {
wwid 36d039ea00005f06a000003c65fa8f3f0
alias Adm-MySQL
}
multipath {
wwid 36d039ea00006891b000004015fa8f58c
alias Adm-OS
}
multipath {
wwid 36d039ea00005f06a000003c55fa8f3e4
alias SN-OS
}
multipath {
wwid 36d039ea00006891b000004035fa8f5a2
alias SN-Db00
}
multipath {
wwid 36d039ea00005f06a000003c75fa8f3fc
alias SN-Db01
}
multipath {
    wwid 36d039ea00006891b000004045fa8f5af
alias SN-Db02
}
multipath {
wwid 36d039ea00005f06a000003c85fa8f40a
alias GW-OS
}
}
----


ホストOSにDockerをインストールする前に、LUNまたはディスクのバッキングをフォーマットしてマウントし `/var/lib/docker`ます。他のLUNはノード構成ファイルに定義され、StorageGRIDコンテナによって直接使用されます。つまり、これらのファイルシステムはホストOSには表示されず、コンテナ自体に表示され、インストーラによって処理されます。

iSCSIベースのLUNを使用している場合は、fstabファイルに次のような行を追加します。前述のように、他のLUNはホストOSにマウントする必要はありませんが、使用可能なブロックデバイスとして表示される必要があります。

[listing]
----
/dev/disk/by-path/pci-0000:03:00.0-scsi-0:0:1:0 /var/lib/docker ext4 defaults 0 0
----


== Dockerのインストールの準備

Dockerのインストールを準備するには、次の手順を実行します。

.手順
. 3つのホストすべてのDockerストレージボリュームにファイルシステムを作成します。
+
[listing]
----
# sudo mkfs.ext4 /dev/sd?
----
+
iSCSIデバイスをマルチパスで使用している場合は、を使用し `/dev/mapper/Docker-Store`ます。

. Dockerストレージボリュームマウントポイントを作成します。
+
[listing]
----
# sudo mkdir -p /var/lib/docker
----
. 同様のエントリをdocker-storage-volume-deviceに追加します `/etc/fstab`。
+
[listing]
----
/dev/disk/by-path/pci-0000:03:00.0-scsi-0:0:1:0 /var/lib/docker ext4 defaults 0 0
----
+
次の `_netdev` オプションは、iSCSIデバイスを使用している場合にのみ推奨されます。ローカルのブロックデバイスを使用する場合は `_netdev` 必要ないため、を推奨します。 `defaults`

+
[listing]
----
/dev/mapper/Docker-Store /var/lib/docker ext4 _netdev 0 0
----
. 新しいファイルシステムをマウントし、ディスクの使用状況を表示します。
+
[listing]
----
# sudo mount /var/lib/docker
[root@host1]# df -h | grep docker
/dev/sdb 200G 33M 200G 1% /var/lib/docker
----
. パフォーマンス上の理由から、スワップをオフにして無効にします。
+
[listing]
----
$ sudo swapoff --all
----
. 設定を維持するには、次のようなすべてのスワップエントリを/etc/fstabから削除します。
+
[listing]
----
/dev/mapper/rhel-swap swap defaults 0 0
----
+

NOTE: スワップを完全に無効にできないと、パフォーマンスが大幅に低下する可能性があります

. ノードのテストリブートを実行して、ボリュームが永続的であり、すべてのディスクデバイスが戻ってくることを確認し `/var/lib/docker` ます。

